{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c65e5b8",
   "metadata": {},
   "source": [
    "# T1 + T2 MRI → Multi-tracer PET Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf5a6a",
   "metadata": {},
   "source": [
    "## 1) Imports & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac285d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\pytorch_3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio as tio\n",
    "from monai.utils import set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchmetrics.functional.image import peak_signal_noise_ratio, structural_similarity_index_measure\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.losses import RelativisticPatchAdversarialLoss\n",
    "from generative.networks.nets import DiffusionModelUNet, PatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4668f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "set_determinism(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd27fb",
   "metadata": {},
   "source": [
    "## 2) Data paths & mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54a9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torchio as tio\n",
    "\n",
    "# -------------------------\n",
    "# Data loading (TorchIO) — matches the original notebook logic\n",
    "# -------------------------\n",
    "DATA_ROOT = Path(\"F:/Dataset/NFLLONG new normalised/all modalities\")\n",
    "MASK_PATH = Path(\"F:/Dataset/NFLLONG new normalised/resized_mask_181_217_181.nii.gz\")\n",
    "\n",
    "# Load brain mask (X, Y, Z) numpy array\n",
    "mask_img = nib.load(str(MASK_PATH))\n",
    "mask = mask_img.get_fdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1dce16",
   "metadata": {},
   "source": [
    "## 3) Build TorchIO Subjects (subject-level split; tracer-conditioned targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496c2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete-modality dataset size: 80 subjects\n",
      "Train samples: 341\n",
      "Sampling samples: 30\n"
     ]
    }
   ],
   "source": [
    "# List subject folders\n",
    "sbj_ids = os.listdir(DATA_ROOT)\n",
    "\n",
    "# 1) Filter subjects with all required modalities\n",
    "subjects_with_complete_modality = []\n",
    "for sbj_id in sbj_ids:\n",
    "    base_path = DATA_ROOT / sbj_id\n",
    "    mod1_list = list(base_path.glob(\"*_T1_CS.nii\"))\n",
    "    mod2_list = list(base_path.glob(\"*_PBR_CS.nii\"))\n",
    "    mod3_list = list(base_path.glob(\"*_PIB_CS.nii\"))\n",
    "    mod4_list = list(base_path.glob(\"*_TAU_CS.nii\"))\n",
    "    mod5_list = list(base_path.glob(\"w*.nii\"))  # auxiliary MRI (e.g., T2-FLAIR)\n",
    "\n",
    "    if all([len(mod1_list) == 1, len(mod2_list) == 1, len(mod3_list) == 1, len(mod4_list) == 1, len(mod5_list) == 1]):\n",
    "        subjects_with_complete_modality.append(\n",
    "            tio.Subject(\n",
    "                source_1=tio.ScalarImage(mod1_list[0]),  # T1\n",
    "                source_2=tio.ScalarImage(mod5_list[0]),  # T2F\n",
    "                modality_2=tio.ScalarImage(mod2_list[0]),  # PBR\n",
    "                modality_3=tio.ScalarImage(mod3_list[0]),  # PIB\n",
    "                modality_4=tio.ScalarImage(mod4_list[0])   # TAU\n",
    "            )\n",
    "        )\n",
    "\n",
    "dataset_with_complete_modality = tio.SubjectsDataset(subjects_with_complete_modality)\n",
    "print(\"Complete-modality dataset size:\", len(dataset_with_complete_modality), \"subjects\")\n",
    "\n",
    "# 2) Choose a held-out subset of physical subjects (optional)\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "val_size = 10\n",
    "indexes = random.sample(range(len(dataset_with_complete_modality)), min(val_size, len(dataset_with_complete_modality)))\n",
    "heldout_sbjs = {str(dataset_with_complete_modality[i][\"source_1\"][\"stem\"])[:5] for i in indexes}\n",
    "\n",
    "# 3) Build per-tracer samples (each subject contributes up to 3 samples: PBR/PIB/TAU)\n",
    "subjects_train, subjects_sample = [], []\n",
    "\n",
    "def append_subject(sbj_id, src_path, tgt_path, t2f_path, label, is_sample):\n",
    "    subject = tio.Subject(\n",
    "        source_1=tio.ScalarImage(src_path),\n",
    "        source_2=tio.ScalarImage(t2f_path),\n",
    "        target_modality=tio.ScalarImage(tgt_path),\n",
    "        label=label\n",
    "    )\n",
    "    # Apply mask (zeros background)\n",
    "    subject[\"source_1\"][\"data\"] *= mask\n",
    "    subject[\"source_2\"][\"data\"] *= mask\n",
    "    subject[\"target_modality\"][\"data\"] *= mask\n",
    "\n",
    "    if is_sample:\n",
    "        subjects_sample.append(subject)\n",
    "    else:\n",
    "        subjects_train.append(subject)\n",
    "\n",
    "for sbj_id in sbj_ids:\n",
    "    is_sample = sbj_id in heldout_sbjs\n",
    "    base_path = DATA_ROOT / sbj_id\n",
    "    try:\n",
    "        t1 = list(base_path.glob(\"*_T1_CS.nii\"))[0]\n",
    "        t2f = list(base_path.glob(\"w*\"))[0]\n",
    "        for label, target_glob in enumerate([\"*_PBR_CS.nii\", \"*_PIB_CS.nii\", \"*_TAU_CS.nii\"]):\n",
    "            target_list = list(base_path.glob(target_glob))\n",
    "            if target_list:\n",
    "                append_subject(sbj_id, t1, target_list[0], t2f, label, is_sample)\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "dataset_train = tio.SubjectsDataset(subjects_train)\n",
    "dataset_sample = tio.SubjectsDataset(subjects_sample)\n",
    "print(\"Train samples:\", len(dataset_train))\n",
    "print(\"Sampling samples:\", len(dataset_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4f191",
   "metadata": {},
   "source": [
    "## 4) Preprocessing transforms & dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceaa84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tio.Compose([\n",
    "    tio.RescaleIntensity(out_min_max=(-1, 1)),\n",
    "    tio.Crop([11, 10, 20, 17, 0, 21]),\n",
    "    tio.Resize((160, 180, 160)),\n",
    "])\n",
    "\n",
    "training_set = tio.SubjectsDataset(dataset_train, transform=transform)\n",
    "sampling_set = tio.SubjectsDataset(dataset_sample, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=1, shuffle=True, num_workers=0)\n",
    "sample_loader = torch.utils.data.DataLoader(sampling_set, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37a658",
   "metadata": {},
   "source": [
    "## 5) Model, discriminator, scheduler, inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22948e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    num_channels=[16, 32, 64],\n",
    "    attention_levels=[False, False, True],\n",
    "    num_head_channels=[0, 0, 64],\n",
    "    num_res_blocks=2,\n",
    "    norm_num_groups=8,\n",
    "    use_flash_attention=True,\n",
    "    with_conditioning=True,\n",
    "    cross_attention_dim=64,\n",
    "    num_class_embeds=4,\n",
    ").to(device)\n",
    "\n",
    "discriminator = PatchDiscriminator(spatial_dims=3, num_layers_d=2, num_channels=4, in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule='scaled_linear_beta', beta_start=5e-4, beta_end=1.95e-2)\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "adv_loss = RelativisticPatchAdversarialLoss(discriminator)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e298ac8",
   "metadata": {},
   "source": [
    "## 6) Checkpoint helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870d7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, save_path, epoch):\n",
    "    save_path = Path(save_path)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch}, save_path)\n",
    "\n",
    "def load_checkpoint(model, load_path):\n",
    "    checkpoint = torch.load(load_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return checkpoint.get('epoch', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3450e",
   "metadata": {},
   "source": [
    "## 7) Training loop (DDPM noise loss + x0 L1; adversarial after warm-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132dfc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████| 341/341 [15:21<00:00,  2.70s/it, loss=1.04, noise_loss=0.635, x0_pred_loss=0.409, gen_loss=0, disc_loss=0, PSNR=12.3, SSIM=0.0967]\n",
      "Epoch 2: 100%|██████| 341/341 [16:07<00:00,  2.84s/it, loss=0.374, noise_loss=0.126, x0_pred_loss=0.177, gen_loss=0.713, disc_loss=0.933, PSNR=18.1, SSIM=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2 # run an example: 1 epoch of warnup and 1 epoch of adverserial learning\n",
    "autoencoder_warm_up_n_epochs = 1\n",
    "adv_weight = 0.1\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "for epoch_ in range(n_epochs):\n",
    "    epoch = epoch_ + 1\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_noise_loss = 0\n",
    "    epoch_x0_pred_loss = 0\n",
    "    gen_epoch_loss = 0\n",
    "    disc_epoch_loss = 0\n",
    "    epoch_psnr = 0\n",
    "    epoch_ssim = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=160)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        source_1 = batch['source_1']['data'].to(device)\n",
    "        source_2 = batch['source_2']['data'].to(device)\n",
    "        images = batch['target_modality']['data'].to(device)\n",
    "        image_class = batch[\"label\"].to(device)\n",
    "        \n",
    "        condition = torch.cat((source_1,source_2),1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            x0_pred = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(inputs=images, diffusion_model=model, label=image_class, noise=noise, timesteps=timesteps, condition=condition) \n",
    "            noised_image = scheduler.add_noise(original_samples = images, noise=noise, timesteps=timesteps)\n",
    "            \n",
    "            for n in range (len(noise_pred)):\n",
    "                _, x0_pred[n] = scheduler.step(torch.unsqueeze(noise_pred[n,:,:,:,:], 0), timesteps[n], torch.unsqueeze(noised_image[n,:,:,:,:], 0))\n",
    "                if image_class[n] == 3:\n",
    "                    x0_pred[n] = x0_pred[n]*0.1\n",
    "                    images[n] = images[n]*0.1\n",
    "\n",
    "            noise_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "            x0_pred_loss = F.l1_loss(x0_pred,images)\n",
    "            loss = noise_loss + x0_pred_loss\n",
    "\n",
    "            if epoch > autoencoder_warm_up_n_epochs:\n",
    "                logits_real = discriminator(images.contiguous().float())[-1]\n",
    "                logits_fake = discriminator(x0_pred.contiguous().float())[-1]\n",
    "                generator_loss = adv_loss(logits_real, logits_fake, images, x0_pred, for_discriminator=False)\n",
    "                loss += adv_weight * generator_loss\n",
    "            \n",
    "            PSNR_mid = peak_signal_noise_ratio(x0_pred, images)\n",
    "            SSIM_mid = structural_similarity_index_measure(x0_pred, images)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if epoch > autoencoder_warm_up_n_epochs:\n",
    "            # Discriminator part\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            logits_fake = discriminator(x0_pred.contiguous().detach())[-1]\n",
    "            logits_real = discriminator(images.contiguous().detach())[-1]\n",
    "            discriminator_loss = adv_loss(logits_real, logits_fake, images, x0_pred, for_discriminator=True)\n",
    "            loss_d = adv_weight * discriminator_loss\n",
    "\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_noise_loss += noise_loss.item()\n",
    "        epoch_x0_pred_loss += x0_pred_loss.item()\n",
    "        if epoch > autoencoder_warm_up_n_epochs:\n",
    "            gen_epoch_loss += generator_loss.item()\n",
    "            disc_epoch_loss += discriminator_loss.item()\n",
    "        epoch_psnr += PSNR_mid.item()\n",
    "        epoch_ssim += SSIM_mid.item()\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1),\n",
    "                                  \"noise_loss\": epoch_noise_loss / (step + 1), \n",
    "                                  \"x0_pred_loss\": epoch_x0_pred_loss / (step + 1),\n",
    "                                  \"gen_loss\": gen_epoch_loss / (step + 1),\n",
    "                                  \"disc_loss\": disc_epoch_loss / (step + 1),\n",
    "                                  \"PSNR\": epoch_psnr / (step + 1),\n",
    "                                  \"SSIM\": epoch_ssim / (step + 1)})\n",
    "    \n",
    "    save_path = 'checkpoints/epoch'+str(epoch)+'_checkpoint.pt'\n",
    "    save_checkpoint(model, save_path, epoch)\n",
    "    \n",
    "    save_path_discriminator = 'discriminator_checkpoints/epoch'+str(epoch)+'_checkpoint.pt'\n",
    "    save_checkpoint(discriminator, save_path_discriminator, epoch)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26478e5",
   "metadata": {},
   "source": [
    "## 8) Sampling / inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71ff72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:55<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthesized outputs to: synth_PBR\n"
     ]
    }
   ],
   "source": [
    "# Sampling / inference + saving synthesized volumes\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import autocast\n",
    "from torchmetrics.functional.image import peak_signal_noise_ratio, structural_similarity_index_measure\n",
    "\n",
    "epoch_to_load = 99\n",
    "ckpt_path = f\"checkpoints/epoch{epoch_to_load}_checkpoint.pt\"\n",
    "epoch_loaded = load_checkpoint(model, ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "# Use a name that doesn't imply a \"validation\" split\n",
    "sample_loader = sample_loader\n",
    "\n",
    "tracer = [\"PBR\", \"*PIB\", \"TAU\"]\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "for step, batch in enumerate(sample_loader):\n",
    "    if step == 0: # run one example\n",
    "        bsz = len(batch['source_1']['path'])\n",
    "\n",
    "        source_1 = batch['source_1']['data'].to(device)\n",
    "        source_2 = batch['source_2']['data'].to(device)\n",
    "        condition = torch.cat((source_1, source_2), dim=1)\n",
    "\n",
    "        ground_truth = batch['target_modality']['data'].to(device)\n",
    "        image_class = batch['label'].to(device)\n",
    "\n",
    "        for seed in range(1): # run one example; change this to the repeating times for MC sampling\n",
    "            SEED = seed\n",
    "            torch.manual_seed(SEED)\n",
    "\n",
    "            input_noise = torch.randn((bsz, 1, 160, 180, 160), device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    pred_PET = inferer.sample(\n",
    "                        input_noise=input_noise,\n",
    "                        diffusion_model=model,\n",
    "                        label=image_class,\n",
    "                        scheduler=scheduler,\n",
    "                        save_intermediates=False,\n",
    "                        intermediate_steps=100,\n",
    "                        conditioning=condition,\n",
    "                    )\n",
    "\n",
    "            # Save one synthesized volume per subject per seed\n",
    "            for i in range(bsz):\n",
    "                subject_id = str(batch['target_modality']['stem'][i])\n",
    "\n",
    "                preds = input_noise[i].detach().cpu()\n",
    "                target = ground_truth[i].detach().cpu()\n",
    "\n",
    "                PSNR = peak_signal_noise_ratio(preds, target)\n",
    "                SSIM = structural_similarity_index_measure(preds, target)\n",
    "                # print(\"PSNR is: \", str(PSNR.item()))\n",
    "                # print(\"SSIM is: \", str(SSIM.item()))\n",
    "\n",
    "                # Save synthesized 3D PET volume\n",
    "                out_dir = Path(f\"synth_{tracer[int(image_class[i].item())]}/\")\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                out_pt = out_dir / f\"syn_{subject_id}_seed{SEED}.pt\"\n",
    "                torch.save(preds, out_pt)\n",
    "\n",
    "print(f\"Saved synthesized outputs to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c192ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
